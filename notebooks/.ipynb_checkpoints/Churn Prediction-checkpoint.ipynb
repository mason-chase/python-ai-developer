{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4283839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a76496",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/WA_Fn-UseC_-Telco-Customer-Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d66dd4",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccf5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['customerID']\n",
    "data = data.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dedbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalCharges'] = data['TotalCharges'].replace('', None)\n",
    "data['TotalCharges'] = data['TotalCharges'].replace(' ', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d28f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalCharges'] = data['TotalCharges'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df9664",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7138c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if len(np.unique(data[column])) == 2:\n",
    "        data[column] = pd.factorize(data[column])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90b7cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_onehot = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "          'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "          'Contract', 'PaymentMethod']\n",
    "\n",
    "for column in to_be_onehot:\n",
    "    # create an instance of OneHotEncoder\n",
    "    ohe = OneHotEncoder()\n",
    "\n",
    "    # use fit_transform() to convert the string variable to one-hot-encoded data\n",
    "    data_encoded = ohe.fit_transform(data[[column]]).toarray()\n",
    "\n",
    "    # create a new DataFrame with the one-hot-encoded data\n",
    "    data_onehot = pd.DataFrame(data_encoded, columns=ohe.get_feature_names_out([column]))\n",
    "    \n",
    "    data = pd.concat([data, data_onehot], axis=1)\n",
    "    data = data.drop(column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07261d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Churn', axis=1).values.astype(float)\n",
    "y = data['Churn'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622418b",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Random Forest is a supervised machine learning algorithm used for classification, regression, and other tasks using decision trees. It is an ensemble learning method that creates a set of decision trees from a randomly selected subset of the training set. The algorithm then collects the votes from different decision trees to decide the final prediction. Random Forest is a powerful algorithm that can handle high-dimensional datasets with many features and can also handle missing data.\n",
    "\n",
    "# Random Forest in Scikit-Learn\n",
    "Scikit-Learn is a popular Python library for machine learning. It provides an implementation of the Random Forest algorithm through the RandomForestClassifier and RandomForestRegressor classes. These classes are built on top of Scikit-Learn's decision tree implementation and provide additional functionality for building and tuning random forest models.\n",
    "\n",
    "To use Random Forest in Scikit-Learn, you can follow these steps:\n",
    "1. Import the necessary modules: \"RandomForestClassifier\" or \"RandomForestRegressor\" from \"sklearn.ensemble\", \"GridSearchCV\" from \"sklearn.model_selection\", and any other necessary modules.\n",
    "\n",
    "2. Define the parameter grid for the hyperparameters you want to tune. For example, you might want to tune the number of trees \"n_estimators\", the maximum depth of each tree \"max_depth\", and the minimum number of samples required to split an internal node \"min_samples_split\".\n",
    "\n",
    "3. Create an instance of the \"RandomForestClassifier\" or \"RandomForestRegressor\" with default hyperparameters.\n",
    "\n",
    "4. Create an instance of \"GridSearchCV\" with the random forest instance, the parameter grid, and any other necessary arguments such as cross-validation folds.\n",
    "\n",
    "5. Fit the \"GridSearchCV\" instance to your training data.\n",
    "\n",
    "6. Access the best estimator using the \"best_estimator_\" attribute of the \"GridSearchCV\" instance.\n",
    "\n",
    "7. Use the best estimator to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2f32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0059c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8a582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search object\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e3b58e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [5, 10, 20],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [50, 100, 200]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit grid search object to data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc990358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Print best hyperparameters\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d46256e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid_search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6ac4949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8031235210601041"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1305d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1409,  143],\n",
       "       [ 273,  288]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5837ea80",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "Gradient Boosting is a machine learning technique used for both regression and classification problems. It is an ensemble method that combines several weak learners into a single strong learner in an iterative fashion. The weak learners are typically decision trees. Gradient Boosting works by iteratively adding decision trees to the model, with each tree correcting the errors of the previous one. The final prediction is the weighted sum of the predictions of all the trees.\n",
    "# Gradient Boosting in Scikit-Learn\n",
    "Scikit-Learn provides an implementation of Gradient Boosting for both regression and classification problems through the \"GradientBoostingRegressor\" and \"GradientBoostingClassifier\" classes, respectively. These classes are built on top of Scikit-Learn's decision tree implementation and provide additional functionality for building and tuning Gradient Boosting models.\n",
    "\n",
    "To use Gradient Boosting in Scikit-Learn, you can follow these steps:\n",
    "1. Import the necessary modules: \"GradientBoostingRegressor\" or \"GradientBoostingClassifier\" from \"sklearn.ensemble\", \"GridSearchCV\" from \"sklearn.model_selection\", and any other necessary modules.\n",
    "\n",
    "2. Define the parameter grid for the hyperparameters you want to tune. For example, you might want to tune the number of trees \"n_estimators\", the maximum depth of each tree \"max_depth\", and the learning rate \"learning_rate\".\n",
    "\n",
    "3. Create an instance of the \"GradientBoostingRegressor\" or \"GradientBoostingClassifier\" with default hyperparameters.\n",
    "\n",
    "4. Create an instance of \"GridSearchCV\" with the Gradient Boosting instance, the parameter grid, and any other necessary arguments such as cross-validation folds.\n",
    "\n",
    "5. Fit the \"GridSearchCV\" instance to your training data.\n",
    "\n",
    "6. Access the best estimator using the \"best_estimator_\" attribute of the \"GridSearchCV\" instance.\n",
    "\n",
    "7. Use the best estimator to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10dcb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd46d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GradientBoostingClassifier instance\n",
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65d52085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(gb, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7bddb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.1, 0.05, 0.01],\n",
       "                         'max_depth': [3, 5, 7],\n",
       "                         'n_estimators': [100, 500, 1000]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c595c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# Print best hyperparameters\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa29cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid_search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5c29d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8050165641268339"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b65947a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1417,  135],\n",
       "       [ 277,  284]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74ffc0",
   "metadata": {},
   "source": [
    "# ExtraTrees Classifier\n",
    "ExtraTrees Classifier is an ensemble tree-based machine learning approach that uses randomization to reduce variance and computational cost compared to Random Forest. It can be used for classification or regression, in scenarios where computational cost is a concern and where the data is not normally distributed. ExtraTrees Classifier creates a group of unpruned decision trees using the traditional top-down method and aggregates the results from the group of decision trees to output a prediction.\n",
    "\n",
    "## Differences between Extra Trees and Random Forest\n",
    "Extra Trees and Random Forest are two similar ensemble methods that construct multiple trees during training time over the entire dataset. However, there are some differences between them. In Extra Trees, it selects a random split to divide the parent node into two random child nodes, while in Random Forest, it selects the best split among a random subset of features. Additionally, Extra Trees constructs trees over every observation in the dataset but with different subsets of features.\n",
    "\n",
    "## Implementing ExtraTrees Classifier in Scikit-Learn\n",
    "Scikit-Learn provides an implementation of ExtraTrees Classifier through the \"ExtraTreesClassifier\" class. To use it, you can follow these steps:\n",
    "\n",
    "1. Import the necessary modules: \"ExtraTreesClassifier\" from \"sklearn.ensemble\", \"GridSearchCV\" from \"sklearn.model_selection\", and any other necessary modules.\n",
    "\n",
    "2. Define the parameter grid for the hyperparameters you want to tune. For example, you might want to tune the number of trees \"n_estimators\", the maximum depth of each tree \"max_depth\", and the minimum number of samples required to split an internal node \"min_samples_split\".\n",
    "\n",
    "3. Create an instance of the \"ExtraTreesClassifier\" with default hyperparameters.\n",
    "\n",
    "4. Create an instance of \"GridSearchCV\" with the \"ExtraTreesClassifier\" instance, the parameter grid, and any other necessary arguments such as cross-validation folds.\n",
    "\n",
    "5. Fit the \"GridSearchCV\" instance to your training data.\n",
    "\n",
    "6. Access the best estimator using the \"best_estimator_\" attribute of the \"GridSearchCV\" instance.\n",
    "\n",
    "7. Use the best estimator to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "320eaada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a481ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ExtraTreesClassifier instance\n",
    "et = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc8ab773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(et, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1061c5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=ExtraTreesClassifier(),\n",
       "             param_grid={'max_depth': [3, 5, 7],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [100, 500, 1000]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2e6433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid_search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a28abe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8106956933270232"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab4ebb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1420,  132],\n",
       "       [ 268,  293]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc7dfd",
   "metadata": {},
   "source": [
    "# Bagging Classifier\n",
    "Bagging Classifier is an ensemble meta-estimator that fits base classifiers on random subsets of the original dataset and then aggregates their individual predictions to form a final prediction. It is an easy-to-use and effective method for improving the performance of a single model. Bagging stands for Bootstrap Aggregating, which is an ensemble machine learning technique that combines the predictions of multiple models to improve the overall performance of the system. The Bagging Classifier uses bootstrap resampling to generate multiple different subsets of the training data\n",
    "## Implementing Bagging Classifier in Scikit-Learn\n",
    "Scikit-Learn provides an implementation of Bagging Classifier through the \"BaggingClassifier\" class. To use it, you can follow these steps:\n",
    "\n",
    "1. Import the necessary modules: \"BaggingClassifier\" from \"sklearn.ensemble\", \"GridSearchCV\" from \"sklearn.model_selection\", and any other necessary modules.\n",
    "\n",
    "2. Define the parameter grid for the hyperparameters you want to tune. For example, you might want to tune the number of trees \"n_estimators\", the maximum depth of each tree \"max_depth\", and the minimum number of samples required to split an internal node \"min_samples_split\".\n",
    "\n",
    "3. Create an instance of the \"BaggingClassifier\" with default hyperparameters.\n",
    "\n",
    "4. Create an instance of \"GridSearchCV\" with the Bagging Classifier instance, the parameter grid, and any other necessary arguments such as cross-validation folds.\n",
    "\n",
    "5. Fit the GridSearchCV instance to your training data.\n",
    "\n",
    "6. Access the best estimator using the \"best_estimator_\" attribute of the \"GridSearchCV\" instance.\n",
    "\n",
    "7. Use the best estimator to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9188d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_samples': [0.5, 1.0],\n",
    "    'max_features': [0.5, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adaed5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BaggingClassifier instance\n",
    "bc = BaggingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e70d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(bc, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0b3bb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=BaggingClassifier(),\n",
       "             param_grid={'max_features': [0.5, 1.0], 'max_samples': [0.5, 1.0],\n",
       "                         'n_estimators': [100, 500, 1000]})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aae015be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid_search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "315feb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795551348793185"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a4c3089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1406,  146],\n",
       "       [ 286,  275]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6395af72",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7429bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'Cs': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a0dcdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LogisticRegressionCV instance\n",
    "lr = LogisticRegressionCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14fa752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(lr, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a9d1469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "12 fits failed out of a total of 18.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 2031, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 2154, in fit\n",
      "    fold_coefs_ = Parallel(\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1060, in _log_reg_scoring_path\n",
      "    coefs, Cs, n_iter = _logistic_regression_path(\n",
      "  File \"/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 800, in _logistic_regression_path\n",
      "    n_iter = np.zeros(len(Cs), dtype=np.int32)\n",
      "TypeError: object of type 'float' has no len()\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.78661272        nan 0.79533524]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gholizadeh/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LogisticRegressionCV(),\n",
       "             param_grid={'Cs': [0.1, 1, 10], 'penalty': ['l1', 'l2']})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "352e8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid_search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4103f010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8154283009938476"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c8aabc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1418,  134],\n",
       "       [ 256,  305]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb33ef",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb5d8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38debbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVC instance\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d4b1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(svc, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8381eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a953ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid_search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfa77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201045b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
