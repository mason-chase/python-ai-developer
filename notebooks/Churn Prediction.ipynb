{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4283839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a76496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('../data/WA_Fn-UseC_-Telco-Customer-Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d66dd4",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccf5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['customerID']\n",
    "data = data.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dedbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalCharges'] = data['TotalCharges'].replace('', None)\n",
    "data['TotalCharges'] = data['TotalCharges'].replace(' ', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d28f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalCharges'] = data['TotalCharges'].astype(float)\n",
    "data = data.dropna()\n",
    "data = data.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df9664",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7138c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if len(np.unique(data[column])) == 2:\n",
    "        data[column] = pd.factorize(data[column])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90b7cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_onehot = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "          'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "          'Contract', 'PaymentMethod']\n",
    "\n",
    "for column in to_be_onehot:\n",
    "    # create an instance of OneHotEncoder\n",
    "    ohe = OneHotEncoder()\n",
    "\n",
    "    # use fit_transform() to convert the string variable to one-hot-encoded data\n",
    "    data_encoded = ohe.fit_transform(data[[column]]).toarray()\n",
    "\n",
    "    # create a new DataFrame with the one-hot-encoded data\n",
    "    data_onehot = pd.DataFrame(data_encoded, columns=ohe.get_feature_names_out([column]))\n",
    "    \n",
    "    data = pd.concat([data, data_onehot], axis=1)\n",
    "    data = data.drop(column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07261d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Churn', axis=1).values.astype(float)\n",
    "y = data['Churn'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622418b",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Random Forest is a supervised machine learning algorithm used for classification, regression, and other tasks using decision trees. It is an ensemble learning method that creates a set of decision trees from a randomly selected subset of the training set. The algorithm then collects the votes from different decision trees to decide the final prediction. Random Forest is a powerful algorithm that can handle high-dimensional datasets with many features and can also handle missing data.\n",
    "\n",
    "# Random Forest in Scikit-Learn\n",
    "Scikit-Learn is a popular Python library for machine learning. It provides an implementation of the Random Forest algorithm through the RandomForestClassifier and RandomForestRegressor classes. These classes are built on top of Scikit-Learn's decision tree implementation and provide additional functionality for building and tuning random forest models.\n",
    "\n",
    "To use Random Forest in Scikit-Learn, you can follow these steps:\n",
    "1. Import the necessary modules: \"RandomForestClassifier\" or \"RandomForestRegressor\" from \"sklearn.ensemble\", \"GridSearchCV\" from \"sklearn.model_selection\", and any other necessary modules.\n",
    "\n",
    "2. Define the parameter grid for the hyperparameters you want to tune. For example, you might want to tune the number of trees \"n_estimators\", the maximum depth of each tree \"max_depth\", and the minimum number of samples required to split an internal node \"min_samples_split\".\n",
    "\n",
    "3. Create an instance of the \"RandomForestClassifier\" or \"RandomForestRegressor\" with default hyperparameters.\n",
    "\n",
    "4. Create an instance of \"GridSearchCV\" with the random forest instance, the parameter grid, and any other necessary arguments such as cross-validation folds.\n",
    "\n",
    "5. Fit the \"GridSearchCV\" instance to your training data.\n",
    "\n",
    "6. Access the best estimator using the \"best_estimator_\" attribute of the \"GridSearchCV\" instance.\n",
    "\n",
    "7. Use the best estimator to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2f32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0059c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8a582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search object\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit grid search object to data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc990358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best hyperparameters\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46256e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = grid_search.best_estimator_\n",
    "\n",
    "pred = rf_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1305d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5837ea80",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "Gradient Boosting is a machine learning technique used for both regression and classification problems. It is an ensemble method that combines several weak learners into a single strong learner in an iterative fashion. The weak learners are typically decision trees. Gradient Boosting works by iteratively adding decision trees to the model, with each tree correcting the errors of the previous one. The final prediction is the weighted sum of the predictions of all the trees.\n",
    "# Gradient Boosting in Scikit-Learn\n",
    "Scikit-Learn provides an implementation of Gradient Boosting for both regression and classification problems through the \"GradientBoostingRegressor\" and \"GradientBoostingClassifier\" classes, respectively. These classes are built on top of Scikit-Learn's decision tree implementation and provide additional functionality for building and tuning Gradient Boosting models.\n",
    "\n",
    "To use Gradient Boosting in Scikit-Learn, you can follow these steps:\n",
    "1. Import the necessary modules: \"GradientBoostingRegressor\" or \"GradientBoostingClassifier\" from \"sklearn.ensemble\", \"GridSearchCV\" from \"sklearn.model_selection\", and any other necessary modules.\n",
    "\n",
    "2. Define the parameter grid for the hyperparameters you want to tune. For example, you might want to tune the number of trees \"n_estimators\", the maximum depth of each tree \"max_depth\", and the learning rate \"learning_rate\".\n",
    "\n",
    "3. Create an instance of the \"GradientBoostingRegressor\" or \"GradientBoostingClassifier\" with default hyperparameters.\n",
    "\n",
    "4. Create an instance of \"GridSearchCV\" with the Gradient Boosting instance, the parameter grid, and any other necessary arguments such as cross-validation folds.\n",
    "\n",
    "5. Fit the \"GridSearchCV\" instance to your training data.\n",
    "\n",
    "6. Access the best estimator using the \"best_estimator_\" attribute of the \"GridSearchCV\" instance.\n",
    "\n",
    "7. Use the best estimator to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GradientBoostingClassifier instance\n",
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d52085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(gb, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bddb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c595c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best hyperparameters\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_best =  grid_search.best_estimator_\n",
    "\n",
    "pred = gb_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c29d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65947a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74ffc0",
   "metadata": {},
   "source": [
    "# ExtraTrees Classifier\n",
    "ExtraTrees Classifier is an ensemble tree-based machine learning approach that uses randomization to reduce variance and computational cost compared to Random Forest. It can be used for classification or regression, in scenarios where computational cost is a concern and where the data is not normally distributed. ExtraTrees Classifier creates a group of unpruned decision trees using the traditional top-down method and aggregates the results from the group of decision trees to output a prediction.\n",
    "\n",
    "## Differences between Extra Trees and Random Forest\n",
    "Extra Trees and Random Forest are two similar ensemble methods that construct multiple trees during training time over the entire dataset. However, there are some differences between them. In Extra Trees, it selects a random split to divide the parent node into two random child nodes, while in Random Forest, it selects the best split among a random subset of features. Additionally, Extra Trees constructs trees over every observation in the dataset but with different subsets of features.\n",
    "\n",
    "## Implementing ExtraTrees Classifier in Scikit-Learn\n",
    "Scikit-Learn provides an implementation of ExtraTrees Classifier through the \"ExtraTreesClassifier\" class. To use it, you can follow these steps:\n",
    "\n",
    "1. Import the necessary modules: \"ExtraTreesClassifier\" from \"sklearn.ensemble\", \"GridSearchCV\" from \"sklearn.model_selection\", and any other necessary modules.\n",
    "\n",
    "2. Define the parameter grid for the hyperparameters you want to tune. For example, you might want to tune the number of trees \"n_estimators\", the maximum depth of each tree \"max_depth\", and the minimum number of samples required to split an internal node \"min_samples_split\".\n",
    "\n",
    "3. Create an instance of the \"ExtraTreesClassifier\" with default hyperparameters.\n",
    "\n",
    "4. Create an instance of \"GridSearchCV\" with the \"ExtraTreesClassifier\" instance, the parameter grid, and any other necessary arguments such as cross-validation folds.\n",
    "\n",
    "5. Fit the \"GridSearchCV\" instance to your training data.\n",
    "\n",
    "6. Access the best estimator using the \"best_estimator_\" attribute of the \"GridSearchCV\" instance.\n",
    "\n",
    "7. Use the best estimator to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320eaada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ExtraTreesClassifier instance\n",
    "et = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ab773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(et, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_best = grid_search.best_estimator_\n",
    "\n",
    "pred = et_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ebb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc7dfd",
   "metadata": {},
   "source": [
    "# Bagging Classifier\n",
    "Bagging Classifier is an ensemble meta-estimator that fits base classifiers on random subsets of the original dataset and then aggregates their individual predictions to form a final prediction. It is an easy-to-use and effective method for improving the performance of a single model. Bagging stands for Bootstrap Aggregating, which is an ensemble machine learning technique that combines the predictions of multiple models to improve the overall performance of the system. The Bagging Classifier uses bootstrap resampling to generate multiple different subsets of the training data\n",
    "## Implementing Bagging Classifier in Scikit-Learn\n",
    "Scikit-Learn provides an implementation of Bagging Classifier through the \"BaggingClassifier\" class. To use it, you can follow these steps:\n",
    "\n",
    "1. Import the necessary modules: \"BaggingClassifier\" from \"sklearn.ensemble\", \"GridSearchCV\" from \"sklearn.model_selection\", and any other necessary modules.\n",
    "\n",
    "2. Define the parameter grid for the hyperparameters you want to tune. For example, you might want to tune the number of trees \"n_estimators\", the maximum depth of each tree \"max_depth\", and the minimum number of samples required to split an internal node \"min_samples_split\".\n",
    "\n",
    "3. Create an instance of the \"BaggingClassifier\" with default hyperparameters.\n",
    "\n",
    "4. Create an instance of \"GridSearchCV\" with the Bagging Classifier instance, the parameter grid, and any other necessary arguments such as cross-validation folds.\n",
    "\n",
    "5. Fit the GridSearchCV instance to your training data.\n",
    "\n",
    "6. Access the best estimator using the \"best_estimator_\" attribute of the \"GridSearchCV\" instance.\n",
    "\n",
    "7. Use the best estimator to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_samples': [0.5, 1.0],\n",
    "    'max_features': [0.5, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaed5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BaggingClassifier instance\n",
    "bc = BaggingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e70d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(bc, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae015be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_best = grid_search.best_estimator_\n",
    "\n",
    "pred = bc_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315feb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6395af72",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic regression is a type of supervised learning algorithm used for classification tasks. It estimates the probability of an event occurring based on one or more independent variables.The output of logistic regression is a binary value (0 or 1, yes/no, true/false).\n",
    "\n",
    "Here are the general steps to perform logistic regression using scikit-learn:\n",
    "\n",
    "1. Import the necessary packages, classes, and functions.\n",
    "2. Load the data.\n",
    "3. Transform the data if necessary.\n",
    "4. Fit the logistic regression model to the data.\n",
    "5. Evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'Cs': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0dcdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LogisticRegressionCV instance\n",
    "lr = LogisticRegressionCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(lr, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best = grid_search.best_estimator_\n",
    "\n",
    "pred = lr_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8aabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb33ef",
   "metadata": {},
   "source": [
    "# Support Vector Classifier\n",
    "Support Vector Classification (SVC) is a type of supervised learning algorithm used for classification tasks. It works by finding the hyperplane that best separates the classes in the feature space. The hyperplane is chosen such that it maximizes the margin between the classes, which is defined as the distance between the hyperplane and the closest data points from each class. The data points that are closest to the hyperplane are known as support vectors.\n",
    "\n",
    "## Implementation in Scikit-learn\n",
    "Scikit-learn provides the \"SVC\" class for fitting the model. The parameters used by this module include:\n",
    "- \"C\": Penalty parameter C of the error term.\n",
    "- \"kernel\": Specifies the kernel type to be used in the algorithm (linear, polynomial, radial basis function (RBF), sigmoid).\n",
    "- \"degree\": Degree of the polynomial kernel function ('poly').\n",
    "- \"gamma\": Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
    "- \"coef)\": Independent term in kernel function.\n",
    "\n",
    "SVC can be used for binary classification problems as well as multi-class classification problems. In scikit-learn, multi-class SVC can be performed using either one-vs-one or one-vs-rest methods.\n",
    "Overall, SVC is a powerful tool for classification tasks and can be easily implemented using scikit-learn's built-in functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38debbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVC instance\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(svc, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8381eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearchCV to training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a953ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best = grid_search.best_estimator_\n",
    "\n",
    "pred = svc_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfa77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eea4b4",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27751db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the VotingClassifier class\n",
    "ensemble_clf = VotingClassifier(estimators=[('rf', rf_best), ('gb', gb_best),\n",
    "                                           ('et', et_best), ('bc', bc_best),\n",
    "                                           ('svc', svc_best)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331977d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svc_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44dd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
